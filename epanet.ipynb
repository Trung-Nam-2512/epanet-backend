{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0c94659a-f09a-4fee-b43e-bc74d8cb2fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, fbeta_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9432adc-a0bb-4a20-a7bd-70f396343f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded:\n",
      "  - USE_FULL_DATASET: True\n",
      "  - ENHANCED_FEATURES: True\n",
      "  - CLASS_WEIGHT_MULTIPLIER: 1.0\n",
      "  - F_BETA: 2.0\n",
      "  - USE_SMOTE: False\n",
      "  - FORCE_GPU: False\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "USE_FULL_DATASET = True\n",
    "ENHANCED_FEATURES = True\n",
    "CLASS_WEIGHT_MULTIPLIER = 1.0  # OPTIMIZED: Gi·∫£m t·ª´ 20.0 xu·ªëng 5.0 ƒë·ªÉ balance Precision v√† Recall\n",
    "F_BETA = 2.0  # F-beta score, beta=2 ∆∞u ti√™n recall g·∫•p ƒë√¥i precision\n",
    "USE_SMOTE = False  # False = NHANH HON v·ªõi dataset l·ªõn, ch·ªâ d√πng class_weight\n",
    "SMOTE_SAMPLE_SIZE = 500000  # Sample tr∆∞·ªõc khi SMOTE n·∫øu USE_SMOTE=True\n",
    "FORCE_GPU = False  # True = force GPU (fail n·∫øu kh√¥ng c√≥), False = auto-detect (recommended)\n",
    "\n",
    "print(\"‚úÖ Configuration loaded:\")\n",
    "print(f\"  - USE_FULL_DATASET: {USE_FULL_DATASET}\")\n",
    "print(f\"  - ENHANCED_FEATURES: {ENHANCED_FEATURES}\")\n",
    "print(f\"  - CLASS_WEIGHT_MULTIPLIER: {CLASS_WEIGHT_MULTIPLIER}\")\n",
    "print(f\"  - F_BETA: {F_BETA}\")\n",
    "print(f\"  - USE_SMOTE: {USE_SMOTE}\")\n",
    "print(f\"  - FORCE_GPU: {FORCE_GPU}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a121d731-8fab-4c58-989e-336fb96f3d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found 1500 scenarios\n",
      "üìÇ Loading 1500 scenarios...\n",
      "  Loaded 100/1500 (1.9s, ~26.3s remaining)\n",
      "  Loaded 200/1500 (2.3s, ~15.1s remaining)\n",
      "  Loaded 300/1500 (2.8s, ~11.2s remaining)\n",
      "  Loaded 400/1500 (3.3s, ~9.1s remaining)\n",
      "  Loaded 500/1500 (3.8s, ~7.7s remaining)\n",
      "  Loaded 600/1500 (4.3s, ~6.5s remaining)\n",
      "  Loaded 700/1500 (5.0s, ~5.7s remaining)\n",
      "  Loaded 800/1500 (5.5s, ~4.9s remaining)\n",
      "  Loaded 900/1500 (6.1s, ~4.1s remaining)\n",
      "  Loaded 1000/1500 (6.6s, ~3.3s remaining)\n",
      "  Loaded 1100/1500 (7.2s, ~2.6s remaining)\n",
      "  Loaded 1200/1500 (7.9s, ~2.0s remaining)\n",
      "  Loaded 1300/1500 (8.5s, ~1.3s remaining)\n",
      "  Loaded 1400/1500 (9.2s, ~0.7s remaining)\n",
      "  Loaded 1500/1500 (9.9s, ~0.0s remaining)\n",
      "‚úÖ Loaded 28,227,000 records in 14.3s\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset_dir = Path(\"dataset\")\n",
    "\n",
    "# Support c·∫£ old structure (file tr·ª±c ti·∫øp) v√† new structure (file trong subdir)\n",
    "parquet_files_direct = sorted(dataset_dir.glob(\"scenario_*.parquet\"))\n",
    "parquet_files_subdir = []\n",
    "for scenario_dir in sorted(dataset_dir.glob(\"scenario_*\")):\n",
    "    if scenario_dir.is_dir():\n",
    "        # New structure: t√¨m nodes.parquet trong subdirectory\n",
    "        nodes_file = scenario_dir / \"nodes.parquet\"\n",
    "        if nodes_file.exists():\n",
    "            parquet_files_subdir.append(nodes_file)\n",
    "        # Ho·∫∑c t√¨m b·∫•t k·ª≥ parquet n√†o trong subdir (fallback)\n",
    "        elif not parquet_files_subdir:\n",
    "            parquet_in_dir = list(scenario_dir.glob(\"*.parquet\"))\n",
    "            if parquet_in_dir:\n",
    "                parquet_files_subdir.extend(parquet_in_dir)\n",
    "\n",
    "parquet_files = sorted(parquet_files_direct + parquet_files_subdir)\n",
    "\n",
    "if len(parquet_files) == 0:\n",
    "    raise FileNotFoundError(\"Khong tim thay parquet files!\")\n",
    "\n",
    "print(f\"‚úÖ Found {len(parquet_files)} scenarios\")\n",
    "\n",
    "max_scenarios = None if USE_FULL_DATASET else 100\n",
    "files_to_load = parquet_files[:max_scenarios] if max_scenarios else parquet_files\n",
    "\n",
    "print(f\"üìÇ Loading {len(files_to_load)} scenarios...\")\n",
    "start_time = time.time()\n",
    "\n",
    "dfs = []\n",
    "for i, f in enumerate(files_to_load):\n",
    "    if (i + 1) % 100 == 0 or (i + 1) == len(files_to_load):\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed if elapsed > 0 else 0\n",
    "        remaining = (len(files_to_load) - i + 1) / rate if rate > 0 else 0\n",
    "        print(f\"  Loaded {i+1}/{len(files_to_load)} ({elapsed:.1f}s, ~{remaining:.1f}s remaining)\")\n",
    "    dfs.append(pd.read_parquet(f))\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "load_time = time.time() - start_time\n",
    "print(f\"‚úÖ Loaded {len(df_all):,} records in {load_time:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5d1afaf0-bb89-42a7-994c-674ed7e8c997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ After filter: 28,081,500 records\n",
      "‚úÖ Junction nodes: 193\n",
      "‚úÖ Reservoir nodes filtered: 1\n"
     ]
    }
   ],
   "source": [
    "# Filter reservoir nodes (ch·ªâ gi·ªØ junction nodes)\n",
    "reservoir_nodes = df_all[df_all['demand'] < -0.1]['node_id'].unique().tolist()\n",
    "df_ml = df_all[~df_all['node_id'].isin(reservoir_nodes)].copy()\n",
    "\n",
    "print(f\"‚úÖ After filter: {len(df_ml):,} records\")\n",
    "print(f\"‚úÖ Junction nodes: {df_ml['node_id'].nunique()}\")\n",
    "print(f\"‚úÖ Reservoir nodes filtered: {len(reservoir_nodes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cb0bf3db-23c4-4357-a33f-56d65768bf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic features created\n",
      "  Leak records: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Basic features\n",
    "df_ml['has_leak'] = (df_ml['leak_demand'] > 0).astype(int)\n",
    "df_ml['hour'] = (df_ml['timestamp'] / 3600).astype(int)\n",
    "df_ml['hour_sin'] = np.sin(2 * np.pi * df_ml['hour'] / 24)\n",
    "df_ml['hour_cos'] = np.cos(2 * np.pi * df_ml['hour'] / 24)\n",
    "\n",
    "# Node ID encoding\n",
    "try:\n",
    "    df_ml['node_id_int'] = pd.to_numeric(df_ml['node_id'], errors='coerce')\n",
    "    if df_ml['node_id_int'].isna().any():\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        le = LabelEncoder()\n",
    "        df_ml['node_id_int'] = le.fit_transform(df_ml['node_id'].astype(str))\n",
    "except:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    df_ml['node_id_int'] = le.fit_transform(df_ml['node_id'].astype(str))\n",
    "\n",
    "print(\"‚úÖ Basic features created\")\n",
    "print(f\"  Leak records: {df_ml['has_leak'].sum():,} ({100*df_ml['has_leak'].mean():.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06fc0c34-c80f-4027-9b31-642889ccf3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Adding enhanced features...\n",
      "‚úÖ Enhanced features added\n",
      "‚úÖ Total features: 16\n",
      "   Features: ['pressure', 'head', 'demand', 'hour_sin', 'hour_cos', 'node_id_int', 'pressure_ma3', 'pressure_ma5', 'head_ma3', 'head_ma5', 'pressure_change', 'head_change', 'pressure_drop', 'head_drop', 'pressure_drop_rate', 'pressure_cumulative_drop']\n"
     ]
    }
   ],
   "source": [
    "# Enhanced features\n",
    "if ENHANCED_FEATURES:\n",
    "    print(\"üîß Adding enhanced features...\")\n",
    "    df_ml = df_ml.sort_values(['node_id', 'timestamp']).reset_index(drop=True)\n",
    "    \n",
    "    # Moving averages\n",
    "    for window in [3, 5]:\n",
    "        df_ml[f'pressure_ma{window}'] = df_ml.groupby('node_id')['pressure'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        df_ml[f'head_ma{window}'] = df_ml.groupby('node_id')['head'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Changes\n",
    "    df_ml['pressure_change'] = df_ml.groupby('node_id')['pressure'].diff().fillna(0)\n",
    "    df_ml['head_change'] = df_ml.groupby('node_id')['head'].diff().fillna(0)\n",
    "    \n",
    "    # Drops\n",
    "    df_ml['pressure_drop'] = df_ml.groupby('node_id')['pressure'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).max() - x\n",
    "    )\n",
    "    df_ml['head_drop'] = df_ml.groupby('node_id')['head'].transform(\n",
    "        lambda x: x.rolling(window=5, min_periods=1).max() - x\n",
    "    )\n",
    "    \n",
    "    # Pressure drop rate\n",
    "    df_ml['pressure_drop_rate'] = df_ml.groupby('node_id')['pressure_change'].transform(\n",
    "        lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "    )\n",
    "    \n",
    "    # Cumulative pressure drop\n",
    "    df_ml['pressure_cumulative_drop'] = df_ml.groupby(['node_id', 'scenario_id'])['pressure_drop'].cumsum()\n",
    "    \n",
    "    print(\"‚úÖ Enhanced features added\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Enhanced features disabled\")\n",
    "\n",
    "# Feature selection\n",
    "basic_features = ['pressure', 'head', 'demand', 'hour_sin', 'hour_cos', 'node_id_int']\n",
    "if ENHANCED_FEATURES:\n",
    "    enhanced_features = [\n",
    "        'pressure_ma3', 'pressure_ma5', 'head_ma3', 'head_ma5',\n",
    "        'pressure_change', 'head_change',\n",
    "        'pressure_drop', 'head_drop',\n",
    "        'pressure_drop_rate', 'pressure_cumulative_drop'\n",
    "    ]\n",
    "    feature_cols = basic_features + enhanced_features\n",
    "else:\n",
    "    feature_cols = basic_features\n",
    "\n",
    "print(f\"‚úÖ Total features: {len(feature_cols)}\")\n",
    "print(f\"   Features: {feature_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1918e0b-119e-489a-8663-efabf22158fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train: 19,657,050 records (1050 scenarios)\n",
      "‚úÖ Val:   4,212,225 records (225 scenarios)\n",
      "‚úÖ Test:  4,212,225 records (225 scenarios)\n",
      "\n",
      "üìä Leak ratio: 0.0000 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Split by scenario\n",
    "scenario_ids = df_ml['scenario_id'].unique()\n",
    "train_scenarios, temp_scenarios = train_test_split(scenario_ids, test_size=0.3, random_state=42)\n",
    "val_scenarios, test_scenarios = train_test_split(temp_scenarios, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = df_ml[df_ml['scenario_id'].isin(train_scenarios)]\n",
    "val_df = df_ml[df_ml['scenario_id'].isin(val_scenarios)]\n",
    "test_df = df_ml[df_ml['scenario_id'].isin(test_scenarios)]\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_df):,} records ({len(train_scenarios)} scenarios)\")\n",
    "print(f\"‚úÖ Val:   {len(val_df):,} records ({len(val_scenarios)} scenarios)\")\n",
    "print(f\"‚úÖ Test:  {len(test_df):,} records ({len(test_scenarios)} scenarios)\")\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['has_leak']\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['has_leak']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['has_leak']\n",
    "\n",
    "leak_ratio = y_train.mean()\n",
    "print(f\"\\nüìä Leak ratio: {leak_ratio:.4f} ({100*leak_ratio:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7032709c-1aa3-48d8-907d-d35397562805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Features normalized using StandardScaler\n"
     ]
    }
   ],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=feature_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=feature_cols, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\"‚úÖ Features normalized using StandardScaler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d0e0ccf-4dcd-4e9e-95a3-5d848af95f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Class distribution:\n",
      "  No Leak: 19,657,050 (100.00%)\n",
      "  Leak:    0 (0.00%)\n",
      "\n",
      "‚ÑπÔ∏è SMOTE disabled - using class_weight only\n",
      "\n",
      "üìä Class weight:\n",
      "   Base class_weight: inf\n",
      "   Adjusted (x1.0): inf\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(f\"üìä Class distribution:\")\n",
    "print(f\"  No Leak: {(y_train == 0).sum():,} ({100*(y_train == 0).mean():.2f}%)\")\n",
    "print(f\"  Leak:    {(y_train == 1).sum():,} ({100*(y_train == 1).mean():.2f}%)\")\n",
    "\n",
    "use_smote = USE_SMOTE\n",
    "if USE_SMOTE:\n",
    "    try:\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        print(\"\\nüîÑ Applying SMOTE...\")\n",
    "        print(f\"   Training data size: {len(X_train_scaled):,} records\")\n",
    "        \n",
    "        # Sample tr∆∞·ªõc n·∫øu dataset qu√° l·ªõn\n",
    "        if SMOTE_SAMPLE_SIZE and len(X_train_scaled) > SMOTE_SAMPLE_SIZE:\n",
    "            print(f\"   Dataset qu√° l·ªõn, sampling {SMOTE_SAMPLE_SIZE:,} records...\")\n",
    "            sample_idx = []\n",
    "            leak_idx = np.where(y_train == 1)[0]\n",
    "            no_leak_idx = np.where(y_train == 0)[0]\n",
    "            \n",
    "            sample_idx.extend(leak_idx.tolist())\n",
    "            \n",
    "            n_no_leak_needed = SMOTE_SAMPLE_SIZE - len(leak_idx)\n",
    "            if n_no_leak_needed > 0:\n",
    "                np.random.seed(42)\n",
    "                sampled_no_leak = np.random.choice(no_leak_idx, size=min(n_no_leak_needed, len(no_leak_idx)), replace=False)\n",
    "                sample_idx.extend(sampled_no_leak.tolist())\n",
    "            \n",
    "            X_train_smote = X_train_scaled.iloc[sample_idx].copy()\n",
    "            y_train_smote = y_train.iloc[sample_idx].copy()\n",
    "            print(f\"   Sampled to {len(X_train_smote):,} records\")\n",
    "        else:\n",
    "            X_train_smote = X_train_scaled\n",
    "            y_train_smote = y_train\n",
    "        \n",
    "        start_smote = time.time()\n",
    "        smote = SMOTE(random_state=42, k_neighbors=3, n_jobs=-1)\n",
    "        X_train_balanced_array, y_train_balanced_array = smote.fit_resample(\n",
    "            X_train_smote.values, y_train_smote.values\n",
    "        )\n",
    "        \n",
    "        X_train_balanced = pd.DataFrame(X_train_balanced_array, columns=X_train_smote.columns)\n",
    "        y_train_balanced = pd.Series(y_train_balanced_array)\n",
    "        \n",
    "        smote_time = time.time() - start_smote\n",
    "        print(f\"‚úÖ SMOTE completed in {smote_time:.1f}s\")\n",
    "        print(f\"   Before: {len(X_train_smote):,} records\")\n",
    "        print(f\"   After:  {len(X_train_balanced):,} records\")\n",
    "        print(f\"   Distribution: No Leak {100*(y_train_balanced == 0).mean():.1f}%, Leak {100*(y_train_balanced == 1).mean():.1f}%\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è imbalanced-learn not installed, skipping SMOTE\")\n",
    "        X_train_balanced = X_train_scaled\n",
    "        y_train_balanced = y_train\n",
    "        use_smote = False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è SMOTE failed: {e}\")\n",
    "        print(\"   Falling back to class_weight only\")\n",
    "        X_train_balanced = X_train_scaled\n",
    "        y_train_balanced = y_train\n",
    "        use_smote = False\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è SMOTE disabled - using class_weight only\")\n",
    "    X_train_balanced = X_train_scaled\n",
    "    y_train_balanced = y_train\n",
    "    use_smote = False\n",
    "\n",
    "# Calculate class weight\n",
    "class_weight_ratio = (y_train_balanced == 0).sum() / (y_train_balanced == 1).sum() if use_smote else (y_train == 0).sum() / (y_train == 1).sum()\n",
    "adjusted_class_weight = class_weight_ratio * CLASS_WEIGHT_MULTIPLIER\n",
    "print(f\"\\nüìä Class weight:\")\n",
    "print(f\"   Base class_weight: {class_weight_ratio:.1f}\")\n",
    "print(f\"   Adjusted (x{CLASS_WEIGHT_MULTIPLIER}): {adjusted_class_weight:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e34f4fe0-31fd-4ffd-8ce9-922a5a23fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß GPU detected (nvidia-smi), testing LightGBM GPU support...\n",
      "‚úÖ GPU tested successfully - using GPU for training!\n",
      "\n",
      "‚úÖ Device selected: GPU\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# GPU Detection\n",
    "device = 'cpu'  # Default\n",
    "if FORCE_GPU:\n",
    "    device = 'gpu'\n",
    "    print(\"üîß FORCE_GPU=True - attempting GPU...\")\n",
    "else:\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)\n",
    "        if result.returncode == 0:\n",
    "            print(\"üîß GPU detected (nvidia-smi), testing LightGBM GPU support...\")\n",
    "            try:\n",
    "                # Test GPU with small dataset\n",
    "                test_model = lgb.LGBMClassifier(n_estimators=1, device='gpu', verbose=-1)\n",
    "                test_model.fit(X_train_balanced[:1000], y_train_balanced[:1000])\n",
    "                device = 'gpu'\n",
    "                print(\"‚úÖ GPU tested successfully - using GPU for training!\")\n",
    "                del test_model\n",
    "            except Exception as gpu_error:\n",
    "                print(f\"‚ö†Ô∏è GPU available but LightGBM GPU test failed: {gpu_error}\")\n",
    "                print(\"   Falling back to CPU (LightGBM CPU is still very fast)\")\n",
    "                device = 'cpu'\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è No GPU detected (nvidia-smi not found) - using CPU\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ÑπÔ∏è nvidia-smi not found - no GPU available, using CPU\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"‚ö†Ô∏è nvidia-smi timeout - using CPU\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ÑπÔ∏è Cannot check GPU ({type(e).__name__}), using CPU: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Device selected: {device.upper()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47596f66-a34f-4d76-b3d6-8a8dda56341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training LightGBM model...\n",
      "   Parameters:\n",
      "     - n_estimators: 1000\n",
      "     - max_depth: 12\n",
      "     - device: GPU\n",
      "     - scale_pos_weight: inf (x1.0)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's binary_logloss: 9.99201e-16\n",
      "\n",
      "‚úÖ Model trained in 11.2s (0.2 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = lgb.LGBMClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    class_weight='balanced',\n",
    "    scale_pos_weight=adjusted_class_weight,\n",
    "    random_state=42,\n",
    "    device=device,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training LightGBM model...\")\n",
    "print(f\"   Parameters:\")\n",
    "print(f\"     - n_estimators: 1000\")\n",
    "print(f\"     - max_depth: 12\")\n",
    "print(f\"     - device: {device.upper()}\")\n",
    "print(f\"     - scale_pos_weight: {adjusted_class_weight:.1f} (x{CLASS_WEIGHT_MULTIPLIER})\")\n",
    "\n",
    "start_train = time.time()\n",
    "\n",
    "model.fit(\n",
    "    X_train_balanced, y_train_balanced,\n",
    "    eval_set=[(X_val_scaled, y_val)],\n",
    "    eval_names=['validation'],\n",
    "    eval_metric='f1',\n",
    "    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(200)]\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_train\n",
    "print(f\"\\n‚úÖ Model trained in {train_time:.1f}s ({train_time/60:.1f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28e749-ab5c-46f0-8ad6-6492c7063d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Accuracy:\n",
      "   Train: 1.0000\n",
      "   Val:   1.0000\n",
      "   Test:  1.0000\n",
      "\n",
      "‚ö†Ô∏è ROC-AUC cannot be calculated (no positive cases in y_test)\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "y_test_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Accuracy\n",
    "train_acc = model.score(X_train_scaled, y_train)\n",
    "val_acc = model.score(X_val_scaled, y_val)\n",
    "test_acc = model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(\"üìä Accuracy:\")\n",
    "print(f\"   Train: {train_acc:.4f}\")\n",
    "print(f\"   Val:   {val_acc:.4f}\")\n",
    "print(f\"   Test:  {test_acc:.4f}\")\n",
    "\n",
    "# ROC-AUC\n",
    "auc_score = None\n",
    "if y_test.sum() > 0:\n",
    "    auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "    print(f\"\\nüìä ROC-AUC: {auc_score:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è ROC-AUC cannot be calculated (no positive cases in y_test)\")\n",
    "    auc_score = 0.0  # Default value for metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9f3ef09-bff9-49a2-ad15-118861d5beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Classification Report (Default Threshold 0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Leak       1.00      1.00      1.00   4212225\n",
      "        Leak       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00   4212225\n",
      "   macro avg       0.50      0.50      0.50   4212225\n",
      "weighted avg       1.00      1.00      1.00   4212225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report (Default Threshold)\n",
    "y_test_pred_default = model.predict(X_test_scaled)\n",
    "print(\"üìä Classification Report (Default Threshold 0.5):\")\n",
    "print(classification_report(y_test, y_test_pred_default, target_names=['No Leak', 'Leak'], labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8af2ca68-da30-4f91-a601-1b3151ee81e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Optimizing threshold for F-2.0 score (∆∞u ti√™n recall)...\n",
      "\n",
      "‚úÖ Best threshold (F-2.0): 0.50\n",
      "   Best F-2.0: 0.0000\n",
      "   Precision: 0.0000\n",
      "   Recall: 0.0000\n",
      "   F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# F-Beta Optimization (∆∞u ti√™n recall)\n",
    "print(f\"üîç Optimizing threshold for F-{F_BETA} score (∆∞u ti√™n recall)...\")\n",
    "\n",
    "test_thresholds = np.arange(0.05, 0.95, 0.05)\n",
    "best_threshold = 0.5\n",
    "best_f_beta = 0\n",
    "best_metrics = {}\n",
    "\n",
    "for thresh in test_thresholds:\n",
    "    y_pred_thresh = (y_test_proba >= thresh).astype(int)\n",
    "    \n",
    "    if y_pred_thresh.sum() == 0:\n",
    "        continue\n",
    "    \n",
    "    f_beta = fbeta_score(y_test, y_pred_thresh, beta=F_BETA)\n",
    "    \n",
    "    if f_beta > best_f_beta:\n",
    "        best_f_beta = f_beta\n",
    "        best_threshold = thresh\n",
    "        \n",
    "        best_metrics = {\n",
    "            'precision': precision_score(y_test, y_pred_thresh),\n",
    "            'recall': recall_score(y_test, y_pred_thresh),\n",
    "            'f1': f1_score(y_test, y_pred_thresh)\n",
    "        }\n",
    "\n",
    "print(f\"\\n‚úÖ Best threshold (F-{F_BETA}): {best_threshold:.2f}\")\n",
    "print(f\"   Best F-{F_BETA}: {best_f_beta:.4f}\")\n",
    "print(f\"   Precision: {best_metrics.get('precision', 0):.4f}\")\n",
    "print(f\"   Recall: {best_metrics.get('recall', 0):.4f}\")\n",
    "print(f\"   F1: {best_metrics.get('f1', 0):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d39e01f-0943-46cf-b156-8d7b864215d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Classification Report (Best Threshold 0.50):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     No Leak       1.00      1.00      1.00   4212225\n",
      "        Leak       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00   4212225\n",
      "   macro avg       0.50      0.50      0.50   4212225\n",
      "weighted avg       1.00      1.00      1.00   4212225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report (Best Threshold)\n",
    "y_test_pred_best = (y_test_proba >= best_threshold).astype(int)\n",
    "print(f\"\\nüìä Classification Report (Best Threshold {best_threshold:.2f}):\")\n",
    "print(classification_report(y_test, y_test_pred_best, target_names=['No Leak', 'Leak'], labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29585b77-7704-49ad-994d-a3ded92f4ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Feature Importance (Top 10):\n",
      "     feature  importance\n",
      "    pressure           0\n",
      "        head           0\n",
      "      demand           0\n",
      "    hour_sin           0\n",
      "    hour_cos           0\n",
      " node_id_int           0\n",
      "pressure_ma3           0\n",
      "pressure_ma5           0\n",
      "    head_ma3           0\n",
      "    head_ma5           0\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    print(\"\\nüìä Feature Importance (Top 10):\")\n",
    "    print(feature_importance.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e1af7-b367-4d78-a0f4-2c2013742c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved: models\\leak_detection_final_20251102_054630.pkl\n",
      "‚úÖ Scaler saved: models\\scaler_final_20251102_054630.pkl\n",
      "‚úÖ Metadata saved: models\\model_metadata_final_20251102_054630.json\n",
      "‚úÖ Created symlinks to latest\n"
     ]
    }
   ],
   "source": [
    "# Prepare metrics\n",
    "report_default = classification_report(y_test, y_test_pred_default, target_names=['No Leak', 'Leak'], labels=[0, 1], output_dict=True)\n",
    "\n",
    "metrics = {\n",
    "    'train_accuracy': float(train_acc),\n",
    "    'val_accuracy': float(val_acc),\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'roc_auc': float(auc_score),\n",
    "    'best_threshold': float(best_threshold),\n",
    "    'best_f_beta': float(best_f_beta),\n",
    "    'precision_leak_default': float(report_default['Leak']['precision']),\n",
    "    'recall_leak_default': float(report_default['Leak']['recall']),\n",
    "    'f1_leak_default': float(report_default['Leak']['f1-score']),\n",
    "    'precision_leak_best': float(best_metrics.get('precision', 0)),\n",
    "    'recall_leak_best': float(best_metrics.get('recall', 0)),\n",
    "    'f1_leak_best': float(best_metrics.get('f1', 0)),\n",
    "}\n",
    "\n",
    "# Save files\n",
    "model_dir = Path(\"models\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_file = model_dir / f\"leak_detection_final_{timestamp}.pkl\"\n",
    "scaler_file = model_dir / f\"scaler_final_{timestamp}.pkl\"\n",
    "metadata_file = model_dir / f\"model_metadata_final_{timestamp}.json\"\n",
    "\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"‚úÖ Model saved: {model_file}\")\n",
    "\n",
    "with open(scaler_file, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"‚úÖ Scaler saved: {scaler_file}\")\n",
    "\n",
    "metadata = {\n",
    "    'model_type': 'lightgbm_final',\n",
    "    'enhanced_features': ENHANCED_FEATURES,\n",
    "    'class_weight_multiplier': CLASS_WEIGHT_MULTIPLIER,\n",
    "    'f_beta': F_BETA,\n",
    "    'feature_cols': feature_cols,\n",
    "    'use_smote': use_smote,\n",
    "    'scale_pos_weight': float(adjusted_class_weight),\n",
    "    'n_scenarios_used': len(files_to_load),\n",
    "    'n_train': len(train_df),\n",
    "    'n_val': len(val_df),\n",
    "    'n_test': len(test_df),\n",
    "    'training_time_seconds': float(train_time),\n",
    "    'leak_ratio': float(leak_ratio),\n",
    "    **metrics\n",
    "}\n",
    "\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úÖ Metadata saved: {metadata_file}\")\n",
    "\n",
    "# Create latest symlinks\n",
    "try:\n",
    "    latest_model = model_dir / \"leak_detection_final_latest.pkl\"\n",
    "    latest_scaler = model_dir / \"scaler_final_latest.pkl\"\n",
    "    latest_metadata = model_dir / \"model_metadata_final_latest.json\"\n",
    "    \n",
    "    for old in [latest_model, latest_scaler, latest_metadata]:\n",
    "        if old.exists():\n",
    "            old.unlink()\n",
    "    \n",
    "    shutil.copy(model_file, latest_model)\n",
    "    shutil.copy(scaler_file, latest_scaler)\n",
    "    shutil.copy(metadata_file, latest_metadata)\n",
    "    print(\"‚úÖ Created symlinks to latest\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not create symlinks: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a04e5f23-7357-4dd1-bc6b-9524c4b86cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "================================================================================\n",
      "Total time: 2.7 minutes\n",
      "\n",
      "üìä Key Metrics:\n",
      "   ROC-AUC: 0.0000\n",
      "   F-2.0 Score: 0.0000\n",
      "   F1-Score: 0.0000\n",
      "   Precision: 0.0000\n",
      "   Recall: 0.0000\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "print(f\"\\nüìä Key Metrics:\")\n",
    "print(f\"   ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "print(f\"   F-{F_BETA} Score: {metrics['best_f_beta']:.4f}\")\n",
    "print(f\"   F1-Score: {metrics['f1_leak_best']:.4f}\")\n",
    "print(f\"   Precision: {metrics['precision_leak_best']:.4f}\")\n",
    "print(f\"   Recall: {metrics['recall_leak_best']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c6930-452e-46ef-a145-9412b0018fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (epanet)",
   "language": "python",
   "name": "epanet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
